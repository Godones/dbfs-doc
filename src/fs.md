# dbfs设计

## DBFS设计：

这种设计是为了满足linux中VFS的路径解析算法，对于一个路径`/d1/dd1/f1`来说，VFS会依次解析`/`、`d1`、`dd1`、`f1`，即从根目录下查找得到目录`d1`，再从`d1`中查找得到目录`dd1`, 如果dbfs需要在vfs的框架下运行，就需要定义良好的数据结构从而可以完成查找文件/目录的功能。

![f2](assert/f2.svg)



上图是使用`jammdb` 数据库的数据结构对文件系统结构的设计图。

在上面所示的图片中,将文件或者目录都使用`bucket`这个数据结构表示，表示文件/目录的`bucket`的编号是唯一的，就像`inode_number`一样，这些`bucket`位于数据库的全局空间中而不是嵌套创建的，位于全局空间的这些`bucket` 在创建时其key就是一个只会递增的`usize`，从而保证了这些`bucket`的唯一性。嵌套的`bucket`可能会被用来存储其它信息，但也可能出于优化考虑不会使用，但保证不会用来标识一个文件/目录。

**说明：下文使用key-data表示存储的value字段是普通的[u8]数组，key-bucket表示出现一个嵌套的`bucket`.**

文件/目录的基础属性用key-data字段来存储，这些基础属性是uid/gid/ctime/atime等。对于文件/目录数据来说，可以使用key-data字段/key-bucket存储。

- 使用key-bucket的结构存储，如果引入这个结构，在表现形式是比较直观，但访问数据时将会发生跨页面的跳转，会导致性能出现下降，这与传统的文件系统实现类似。并且，由于使用bucket来表示一个文件/目录，而前文提到，一个bucket就代表一棵b+树，其至少会包含一个固定大小的存储页，如果这里将数据再放入嵌套的bucket中，将会出现这个bucket空间浪费的情况。

- 将其作为key-data字段，可以保证在文件较小时，文件的属性信息，存储的数据都位于同一个页面上，可以充分利用存储页。**因此这是一种较好的方案。**但是文件和目录存储的内容存在差异，因此如何将数据使用key-data存储起来，还需要更进一步讨论。

对于目录或者文件来说，其本质上来说没有区别，都只是存储数据的一个容器，只是按照传统的说法，目录存储的数据是其子目录或者子文件的索引信息，而文件存储的是用户实际想要存储的数据。bucket这个数据结构将文件和目录统一表示，两者存储的数据都是key-data对，一种直观的想法是:

1. 对于文件来说，其保存的是用户实际数据，那么它的字段可以是

   - `data: [u8]`, 但是如果这样存储文件数据，当文件发生写操作的时候，由于一般key-value数据库不会提供修改的功能，只有删除和添加的操作，因此对于大文件来说，修改就意味着先删除再插入的动作，相当于拷贝了两次数据，这是非常降低性能的操作。

   - `data1: [u8]` `data2: [u8]` `data3: [u8]` ,这种存储优化方式是对数据进行分片,将数据存储为一个个小片，每次对文件进行追加就插入一条`data{i} :[u8]`, 这里的i会一直递增。 但是这也会造成一种现象，即如果追加的数据很少，只有几个bytes, 那么新建一个key-data对就不划算了， 所以再进一步的优化是 规定一个`data` 存储一个固定大小的数据，比如512byte, 当追加数据的时候，如果最后一个`data{i}`未满512bytes 就在此基础上追加，否则就插入新的key-value对。这样每次修改的数据量就可以控制在可以接受的范围内。在这种设计下，当文件的读写发生在中间部分时，会出现数据跨key-value对的情况，但比起直接一个key-data存储文件数据，这种设计会减少很多不必要的开销。

2. 对于目录来说，其保存的是子文件/子目录，假设其包含文件f1,f2,子目录d1,那么它可以存储三个字段:

   `f1: 1`  `f2: 2`  `f3: 3`, 包含的文件名称作为key, 其对应的bucket 编号为 data，这种想法也很直观，但和文件的结构差别大了，我们可以换一种方式，使用

   `data{i}: f1:1`   `data{i}: f2:2`, 即把子文件的名称和其对应的bucket编号存在data字段中，而key字段就与文件存储一样用`datai`标识，只是对于目录来说，就不用考虑将data字段固定一个大小了，因为子文件/目录的名称不会很大，即使发生重命名操作，只需要先删除再添加就可以了。

3. 对于符号链接文件来说，其存储与上面两者类似，但其可能只需要一个`data: path`就可以了。

经过上述的分析，就可以确定文件/目录如何使用key-value组织内容了。

上述的这种设计可以方便地完成硬链接/软连接的实现。

为了完成上述的实现，可能需要一些额外字段的辅助，这时原本可能不需要在数据库中出现的超级块结构就有存在的必要了，比如对于全局空间递增的bucket编号，我们需要记录上一次使用DBFS文件系统后的编号，从而可以在下一次挂载时可以正确创建新的文件。上面对文件的数据存储规定的固定切片大小也可以保存在超级块结构中。而这个所谓超级块只需要一个bucket结构存储即可。



### 一些常见操作的实现

**create file/dir** : 数据库根据填充的`inode_number`的标识号找到对应的bucket, 添加一条`data{i}: f:number` 键值对，并创建一个新的bucket，填充属性信息与控制信息。

**mv file/dir** :  数据库根据旧目录的标识号找到对应bucket，从bucket中删除对应的`data{i}: f:number` ,在新目录下插入一条新的键值对, 新key-data的data字段的number是旧的number

**cp file/dir** ： 数据库根据源目录的标识号找到对应bucket，查找需要复制的文件的键值对，得到其唯一标识符后，再查找标识符得到文件的bucket。 在目标目录下插入一条键值对，新建一个bucket,填充信息，并将复制的文件的数据拷贝至新的bucket中。

**link/unlink:**  数据库根据源目录的标识号找到对应bucket，找到存储对应文件的key-data字段，读出其对应的bucket标识号，新建一条key-data字段存储这个标识号，根据标识号找到对应的bucket，修改保存硬链接数目的key-data字段。

这些操作与传统的文件系统没有太大的差别。只是在数据库中这些操作得到了简化，因为只是键值对的删除和插入操作。



