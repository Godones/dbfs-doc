# fio与filebench

## Fio

fio是一款基于命令行的磁盘I/O性能测试工具，可以测试各种不同类型的磁盘I/O工作负载，例如随机读写、顺序读写、混合读写等。fio可以在Linux、FreeBSD、macOS等操作系统上运行，支持多线程、异步I/O和多种I/O引擎，具有高度的可定制性和可扩展性。

fio的主要优势在于其灵活性和可配置性。用户可以通过fio的配置文件或者命令行选项，自定义测试工作负载、块大小、I/O引擎、线程数、测试时间等参数。此外，fio还可以输出详细的测试结果，包括吞吐量、IOPS、延迟等指标，方便用户进行性能分析和比较。

fio的主要参数有：

- `--name`：用户指定的测试名称，会影响测试文件名
- `--directory`：测试目录
- `--ioengine`：测试时下发 IO 的方式；通常用 libaio 即可
- `--rw`：常用的有 read，write，randread，randwrite，分别代表顺序读写和随机读写
- `--bs`：每次 IO 的大小
- `--size`：每个线程的 IO 总大小；通常就等于测试文件的大小
- `--numjobs`：测试并发线程数；默认每个线程单独跑一个测试文件
- `--direct`：在打开文件时添加 `O_DIRECT` 标记位，不使用系统缓冲，可以使测试结果更稳定准确

为了屏蔽操作系统缓存带来的影响，在进行读写吞吐量进行测试时，将读写的文件设置为内存容量的2倍。并且在测试读性能时，不能紧跟着写测试，因为写测试会填充操作系统的缓存,因此在写入完成之后，需要使用bash命令将操作系统缓存写回到磁盘中。fio的测试包含了单线程和多线程两个测试，在多线程测试中，开启4个线程，每个线程读写3g大小的文件。 每次读写的块大小设置为1MB，设置direct=1忽略系统缓冲。

![fio-test-1job](assert/fio-test-1job.svg)

上图显示了单线程测试结果。在顺序写中，ext3的性能最好，几乎达到了原生读写的速度。而ext4的性能只是ext3的25%，DBFS的性能是EXT3的90%左右，是ext4的3倍，说明DBFS的写性能在一定程度上是可以追赶上ext系列文件系统的。在顺序读中，ext3 的读取速度仍然是最快的，而ext4的读取速度是其89%左右，对于DBFS来说，其读性能相较于两个系统都比较差，只达到了ext4的15%左右。在随机写测试中，ext3与DBFS都保持了与顺序写差不多的性能，性能损失在10%以内。对于ext4,其性能损失达到了35%左右。DBFS的随机写性能是EXT4的3倍，是ext3的60%左右。在随机读测试中，EXT3和EXT4都有较大程度的性能下降，但两者的差距并不是很大，ext4是ext3的80%左右。对于DBFS，其性能与顺序读有相同的性能数据。由于ext3和ext4的性能下降，反而让DBFS的性能超过了两者的20%，但整体性能依然比较落后。

![fio-test-1job-4job](assert/fio-test-1job-4job.svg)

![fio-test-4job](assert/fio-test-4job.svg)

上图显示了并发读写的结果。在并发读写测试中，从右图可以看到，不管是对于顺序读写还是随机读写，三个文件系统都有一定的性能提升，在写测试中三者的性能提升在10%以内，在读测试中ext两者有显著的性能提升，甚至达到了单线程下的1.4倍。对于DBFS来说，其读测试与单线程下的没有提升，其与ext的性能差距被放大，说明DBFS在并发控制方面的能力比较薄弱。



在读写测试中可以看到，在写操作下DBFS是可以超过ext4的性能的，甚至与ext3在同一水平。而在读操作下DBFS的性能会急剧下降，以至于可能只有两者的15%。造成这个性能差距的原因涉及到了诸多方面，其一，DBFS的fuse实现本身就与ext的实现存在实现细节的差异，DBFS对fuse没有做更多的优化，而ext3和ext4的fuse实现是有针对性的优化措施的，比如在进行数据读写时减少数据的拷贝，直接在内核和用户态传递数据，这在读操作时性能提升尤为明显，在DBFS中，一次读操作需要两次拷贝数据，一次从数据库中拷贝到申请的内存中，一次从申请的内存中传递到内核中，对于读15GB数据的测试来说，相当于DBFS需要读取30GB大小的数据;其二，在DBFS中，使用了键值对来存储文件数据，这导致这些数据可能会分散在不同的页面中，而且这些数据可能跨过多个页面，而在ext文件系统中，文件在存储时倾向于将这些数据存放在连续的块中，在进行读取操作时，ext文件系统可以直接快速查找到文件的数据存储区域，而在DBFS中，每一次的查找都会数据库可能都会遍历存储的所有键值对，即使使用树结构可以加速查找过程，但频繁的查找相比只需要几次查找仍然带来了巨大的开销。同时，由于ext的数据存储在连续的块中，这可以减少缓存失效，而DBFS，由于需要进行索引，因此需要频繁地进行缺页处理，而这些页面会在缓存中不断地切换，同样这也会造成性能下降，在写测试中，性能数据产生的波动应该来自于此;其三，在并发测试中，DBFS的写性能提升很小，读性能几乎没有变化，在实现中，每一次写操作都会变成一个写事务，而数据库的写事务将会使得文件被加锁，由于数据库的加锁粒度过大，导致在并发情况下数据库的写操作变成了跟单线程一样的串行操作。对于读操作来说，按理来说读事务不没有对文件加锁，因此其性能应该有较大的提升，但实验中并没有发生，其原因因为时间有限的关系没有进行输入的分析，初步的推断应该与频繁的换页处理有关。



![fio_barh](assert/fio_barh.svg)

虽然DBFS在大文件的写性能测试中达到了与ext3相同的水平，但实验发现，DBFS在写性能上仍然存在改进的空间，上图显示了重复写大文件的情况，在第一次写入大文件时，其性能确实与ext3位于同一水平，但当望这个大文件再次写入数据时，其性能就会下降到第一次的1/4左右。造成这个现象的原因是来自数据库本身的机制，由于使用键值对进行存储数据，在第二次写数据时，需要覆盖第一次写的数据，相当于需要删除原来的一个键并插入一个新的键，而这些键值对存储在页面中，一个键的插入或删除会导致有B+树组织的页面发生分裂或者合并，当这个操作发生频繁时，将会导致性能的快速下降。虽然现实场景中往一个大型文件中重复写入数据不常见，但这确实是一个需要改进的地方。

![small file test](assert/small%20file%20test.svg)

在大文件的测试中DBFS没有表现出非常优秀的性能，但因为数据库具有自己的缓存机制，这使得在小文件的测试中DBFS是可以具有与ext系列相同的性能的，上图显示了在小文件中的结果。同时，因为数据库具有事务特性，因此DBFS中原生地也会支持事务特性。这个特性可以保证用户的数据被安全地存储而不会丢失。	



## Filebench

Filebench是一个基准测试工具，用于评估文件系统和存储系统的性能。它可以模拟各种应用程序的工作负载，包括Web服务器、邮件服务器、数据库服务器、多媒体应用程序等。Filebench支持多种文件系统。其基于脚本的工作方式，让用户可以使用脚本来描述测试场景和工作负载。用户可以使用Filebench提供的预定义脚本，也可以创建自己的脚本来模拟自定义的工作负载。

在本文中，主要使用Web服务器、邮件服务器、文件服务器对文件系统进行了测试。在web服务器中，其目标是模拟简单的网络服务器 I/O 活动。主要的操作是在目录树中的多个文件上生成一系列打开-读取-关闭以及日志文件追加。默认使用 100 个线程。在邮件服务器中，其目标是模拟将每封电子邮件存储在单独文件 (/var/mail/server) 中的简单邮件服务器的 I/O 活动。工作负载由单个目录中的一组多线程执行创建-追加-同步、读取-追加-同步、读取和删除操作。默认使用16个线程。生成的工作负载与 Postmark 有点相似。在文件服务器中，其目标是模拟简单的文件服务器 I/O 活动。此工作负载对目录树执行一系列创建、删除、附加、读取、写入和属性操作。默认使用 50 个线程。生成的工作负载有点类似于 SPECsfs。

![filebench-iop](assert/filebench-iop.svg)

![filebench-throughput](assert/filebench-throughput.svg)

上图显示了测试结果。在filebench的测试中，ext文件系统在web服务器与文件服务器的吞吐量和IOP上相比DBFS具有领先优势，在mailserver中，DBFS相较于两者具有领先优势。

对于这几种工作负载，其更多的操作集中在元数据的处理上，而前文中提到，在ext文件系统的fuse实现中，对于元数据有针对性的优化，同时，这两种文件系统对并发也有较好的支持，两者的读性能也优于DBFS，因此在web服务器和文件服务器中ext的性能会遥遥领先。在邮件服务器中，之所以DBFS具有一定的领先优势，这里主要是因为邮件服务器中所有文件大小总共才占据1.5GB的大小，而这个大小足以让操作系统将文件数据缓存到内存中，在DBFS中，数据库的读事务发生时读取的是内存映射区域，因此文件内容全部被加载到缓存中大大减少DBFS的页交换次数，从而在一定程度上缓解了其性能损失。

filebench的测试进一步说明了DBFS的优势集中在小文件存储、文件搜索、目录查询等操作上。
